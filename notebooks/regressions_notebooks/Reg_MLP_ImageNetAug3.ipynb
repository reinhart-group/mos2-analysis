{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa8adc9",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144103d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "from codes.utils import stratified_train_test_group_kfold\n",
    "from codes.utils import model_test_classification\n",
    "from codes.utils import accuracy_classification\n",
    "from codes.utils import mlp_reg_cross_val_final_test\n",
    "\n",
    "from codes.regression_codes import mlp_regression_gridsearch\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9eddff",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e11c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sampleId', 'sampleLabel', 'image', 'T', 'ImageNet', 'MicroNet'], dtype='object')\n",
      "<class 'generator'>\n",
      "(235, 100)\n",
      "(235,)\n",
      "(27, 100)\n",
      "(27,)\n"
     ]
    }
   ],
   "source": [
    "with open(\"Data/MoS2_Analysis_Data_trained2\", \"rb\") as fp:   # Unpickling\n",
    "    MoS2_Proj1_Class_Data = pickle.load(fp)\n",
    "\n",
    "#features = MoS2_ImageNet_100_data[0]\n",
    "df = pd.DataFrame(MoS2_Proj1_Class_Data)\n",
    "print(df.keys())\n",
    "T_target = np.array(list(df['T']))\n",
    "features = np.array(list(df['ImageNet']))\n",
    "sampleId = np.array(list(df['sampleId']))\n",
    "\n",
    "\n",
    "X = features\n",
    "Y = np.array(T_target)\n",
    "\n",
    "groups = np.array(sampleId)\n",
    "\n",
    "\n",
    "train_val_groups, train_val_X, train_val_Y, test_X, test_Y = stratified_train_test_group_kfold(X, Y, groups, n_splits=10, test_fold=0)\n",
    "\n",
    "\n",
    "#train_val_Y = train_val_Y.flatten()\n",
    "#val_Y = val_Y.flatten()\n",
    "#test_Y = test_Y.flatten()\n",
    "\n",
    "\n",
    "print(train_val_X.shape)\n",
    "print(train_val_Y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_Y.shape)\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy='not majority')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d82cf7",
   "metadata": {},
   "source": [
    "# 3. Runing Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b8b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'Models/regression/ImageNet/aug3/MLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4251c467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "Epoch1: loss: 846912.8750 val_loss: 675364.5000\n",
      "Epoch2: loss: 331703.5553 val_loss: 26980.2324\n",
      "Epoch3: loss: 94075.4768 val_loss: 10814.2441\n",
      "Epoch4: loss: 40751.1094 val_loss: 25436.9980\n",
      "Epoch5: loss: 20810.0592 val_loss: 23927.9180\n",
      "Epoch6: loss: 16438.7070 val_loss: 11764.0527\n",
      "Epoch7: loss: 13140.1489 val_loss: 11098.8203\n",
      "Epoch8: loss: 11089.7662 val_loss: 9432.6035\n",
      "Epoch9: loss: 10264.8479 val_loss: 9677.7695\n",
      "Epoch10: loss: 9268.7883 val_loss: 8931.3770\n",
      "Epoch11: loss: 8572.7666 val_loss: 9113.9512\n",
      "Epoch12: loss: 7978.0914 val_loss: 8790.5850\n",
      "Epoch13: loss: 7646.9170 val_loss: 8644.7949\n",
      "Epoch14: loss: 7203.4300 val_loss: 7915.0332\n",
      "Epoch15: loss: 6829.4598 val_loss: 8515.2607\n",
      "Epoch16: loss: 6604.4916 val_loss: 7455.2812\n",
      "Epoch17: loss: 6163.1127 val_loss: 7950.0894\n",
      "Epoch18: loss: 6027.4669 val_loss: 6883.5381\n",
      "Epoch19: loss: 5724.0151 val_loss: 7010.8594\n",
      "Epoch20: loss: 5474.7882 val_loss: 6407.1250\n",
      "Epoch21: loss: 5523.0283 val_loss: 6891.2554\n",
      "Epoch22: loss: 5185.6632 val_loss: 5861.2417\n",
      "Epoch23: loss: 5085.5912 val_loss: 5694.4893\n",
      "Epoch24: loss: 4883.5222 val_loss: 6136.0483\n",
      "Epoch25: loss: 4832.0940 val_loss: 5219.7148\n",
      "Epoch26: loss: 4637.4846 val_loss: 5208.9175\n",
      "Epoch27: loss: 4541.2950 val_loss: 5486.8701\n",
      "Epoch28: loss: 4659.6631 val_loss: 4591.5059\n",
      "Epoch29: loss: 4128.8934 val_loss: 4594.3447\n",
      "Epoch30: loss: 4049.3987 val_loss: 4369.5713\n",
      "Epoch31: loss: 3982.3855 val_loss: 4170.7266\n",
      "Epoch32: loss: 3854.2144 val_loss: 4197.4023\n",
      "Epoch33: loss: 3803.1093 val_loss: 3889.1904\n",
      "Epoch34: loss: 3715.2218 val_loss: 3805.4277\n",
      "Epoch35: loss: 3647.9300 val_loss: 4039.3542\n",
      "Epoch36: loss: 3558.9672 val_loss: 3554.0771\n",
      "Epoch37: loss: 3480.8702 val_loss: 3504.9927\n",
      "Epoch38: loss: 3342.2971 val_loss: 3836.6074\n",
      "Epoch39: loss: 3403.0165 val_loss: 3436.2954\n",
      "Epoch40: loss: 3355.3815 val_loss: 3503.7812\n",
      "Epoch41: loss: 3236.8712 val_loss: 3328.3435\n",
      "Epoch42: loss: 3163.9969 val_loss: 3131.1011\n",
      "Epoch43: loss: 3179.8575 val_loss: 3153.4333\n",
      "Epoch44: loss: 3002.9088 val_loss: 3198.8154\n",
      "Epoch45: loss: 2972.4893 val_loss: 2892.8005\n",
      "Epoch46: loss: 2885.1469 val_loss: 3292.8828\n",
      "Epoch47: loss: 3044.4311 val_loss: 2796.3708\n",
      "Epoch48: loss: 2849.9564 val_loss: 2761.4617\n",
      "Epoch49: loss: 2854.7506 val_loss: 2729.2107\n",
      "Epoch50: loss: 2763.7155 val_loss: 3077.5464\n",
      "Epoch51: loss: 2636.2350 val_loss: 2680.1841\n",
      "Epoch52: loss: 2824.7629 val_loss: 2570.2200\n",
      "Epoch53: loss: 2792.3592 val_loss: 2672.3232\n",
      "Epoch54: loss: 2703.1081 val_loss: 3170.4893\n",
      "Epoch55: loss: 2678.2147 val_loss: 3306.7109\n",
      "Epoch56: loss: 2608.2554 val_loss: 2489.2417\n",
      "Epoch57: loss: 2518.9542 val_loss: 2562.2891\n",
      "Epoch58: loss: 2558.0881 val_loss: 2560.4976\n",
      "Epoch59: loss: 2512.3607 val_loss: 2429.0649\n",
      "Epoch60: loss: 2388.2048 val_loss: 2644.1489\n",
      "Epoch61: loss: 2414.1707 val_loss: 2496.2341\n",
      "Epoch62: loss: 2651.9523 val_loss: 2922.9453\n",
      "Epoch63: loss: 2440.9888 val_loss: 3752.5977\n",
      "Epoch64: loss: 2484.7292 val_loss: 2840.0923\n",
      "Epoch65: loss: 2515.4009 val_loss: 3141.1074\n",
      "Epoch66: loss: 2493.1875 val_loss: 2873.6455\n",
      "Epoch67: loss: 2274.8633 val_loss: 2509.5264\n",
      "Epoch68: loss: 2165.9423 val_loss: 2562.6975\n",
      "Epoch69: loss: 2263.9886 val_loss: 2519.5798\n",
      "Epoch70: loss: 2202.3772 val_loss: 2571.0552\n",
      "Epoch71: loss: 2261.6293 val_loss: 2714.1921\n",
      "Epoch72: loss: 2195.0511 val_loss: 2661.9897\n",
      "Epoch73: loss: 2110.3568 val_loss: 2536.5085\n",
      "Epoch74: loss: 2151.5359 val_loss: 2532.7847\n",
      "Epoch75: loss: 2065.3655 val_loss: 2604.7598\n",
      "Epoch76: loss: 2070.3834 val_loss: 2863.4785\n",
      "Epoch77: loss: 2083.4361 val_loss: 2563.8008\n",
      "Epoch78: loss: 2087.6581 val_loss: 2708.3882\n",
      "Epoch79: loss: 2075.9644 val_loss: 2750.1560\n",
      "Epoch80: loss: 1993.3052 val_loss: 2814.8496\n",
      "Early stopped training at epoch 80\n",
      "Accuracy of the network on the 315 train images: 54.6 %\n",
      "Accuracy of the network on the 24 val images: 54.2 %\n",
      "fold: 0 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 850395.4313 val_loss: 698039.2500\n",
      "Epoch2: loss: 349607.1758 val_loss: 31312.3828\n",
      "Epoch3: loss: 82282.3268 val_loss: 21987.1699\n",
      "Epoch4: loss: 36881.0092 val_loss: 29796.2109\n",
      "Epoch5: loss: 21419.8268 val_loss: 21635.9824\n",
      "Epoch6: loss: 16167.9458 val_loss: 13027.0117\n",
      "Epoch7: loss: 13840.6606 val_loss: 10894.5723\n",
      "Epoch8: loss: 12555.6179 val_loss: 9575.7598\n",
      "Epoch9: loss: 10573.1637 val_loss: 9467.7070\n",
      "Epoch10: loss: 9681.8591 val_loss: 8565.3398\n",
      "Epoch11: loss: 8973.1799 val_loss: 8402.8633\n",
      "Epoch12: loss: 8341.5467 val_loss: 7848.1968\n",
      "Epoch13: loss: 7755.9281 val_loss: 7335.5215\n",
      "Epoch14: loss: 7361.1760 val_loss: 7428.9971\n",
      "Epoch15: loss: 7017.1519 val_loss: 6774.1572\n",
      "Epoch16: loss: 6731.5921 val_loss: 6662.0659\n",
      "Epoch17: loss: 6296.2400 val_loss: 6304.3066\n",
      "Epoch18: loss: 6021.8398 val_loss: 6292.7905\n",
      "Epoch19: loss: 5797.3869 val_loss: 6335.0977\n",
      "Epoch20: loss: 5526.6434 val_loss: 5782.8828\n",
      "Epoch21: loss: 5433.6580 val_loss: 5533.4111\n",
      "Epoch22: loss: 5122.2166 val_loss: 6278.5146\n",
      "Epoch23: loss: 5259.9904 val_loss: 5305.3086\n",
      "Epoch24: loss: 4942.4505 val_loss: 5159.8330\n",
      "Epoch25: loss: 4600.1111 val_loss: 5036.8145\n",
      "Epoch26: loss: 4397.9900 val_loss: 4878.1987\n",
      "Epoch27: loss: 4244.2799 val_loss: 4868.2969\n",
      "Epoch28: loss: 4126.1676 val_loss: 4727.8740\n",
      "Epoch29: loss: 4051.9055 val_loss: 4736.0400\n",
      "Epoch30: loss: 3938.1626 val_loss: 4655.6885\n",
      "Epoch31: loss: 3799.5823 val_loss: 4500.8735\n",
      "Epoch32: loss: 3727.9173 val_loss: 4421.5898\n",
      "Epoch33: loss: 3560.1119 val_loss: 4562.7246\n",
      "Epoch34: loss: 3585.2858 val_loss: 4517.0581\n",
      "Epoch35: loss: 3419.0622 val_loss: 4370.7041\n",
      "Epoch36: loss: 3270.2379 val_loss: 4590.5693\n",
      "Epoch37: loss: 3391.2245 val_loss: 4247.6221\n",
      "Epoch38: loss: 3181.3270 val_loss: 4296.0405\n",
      "Epoch39: loss: 3177.3941 val_loss: 4183.5586\n",
      "Epoch40: loss: 3054.4744 val_loss: 4188.0127\n",
      "Epoch41: loss: 2978.4963 val_loss: 4123.6255\n",
      "Epoch42: loss: 2913.1521 val_loss: 4229.6226\n",
      "Epoch43: loss: 2827.5474 val_loss: 4295.3057\n",
      "Epoch44: loss: 2875.6612 val_loss: 4162.6074\n",
      "Epoch45: loss: 2737.2862 val_loss: 4257.6294\n",
      "Epoch46: loss: 2748.9986 val_loss: 4183.8701\n",
      "Epoch47: loss: 2664.9274 val_loss: 4396.5264\n",
      "Epoch48: loss: 2798.3441 val_loss: 4380.9658\n",
      "Epoch49: loss: 2600.1608 val_loss: 4238.7544\n",
      "Epoch50: loss: 2569.0415 val_loss: 4251.5127\n",
      "Epoch51: loss: 2654.6643 val_loss: 4294.7939\n",
      "Epoch52: loss: 2692.4114 val_loss: 4677.3613\n",
      "Epoch53: loss: 2476.7939 val_loss: 4314.7495\n",
      "Epoch54: loss: 2391.5585 val_loss: 4282.7285\n",
      "Epoch55: loss: 2390.8264 val_loss: 4338.5728\n",
      "Epoch56: loss: 2501.8740 val_loss: 4347.9346\n",
      "Epoch57: loss: 2420.3612 val_loss: 4387.9580\n",
      "Epoch58: loss: 2279.7463 val_loss: 4277.1748\n",
      "Epoch59: loss: 2281.7589 val_loss: 4318.0830\n",
      "Epoch60: loss: 2243.4791 val_loss: 4772.8647\n",
      "Epoch61: loss: 2369.6439 val_loss: 4414.9883\n",
      "Epoch62: loss: 2408.6462 val_loss: 4279.6035\n",
      "Early stopped training at epoch 62\n",
      "Accuracy of the network on the 318 train images: 52.8 %\n",
      "Accuracy of the network on the 24 val images: 66.7 %\n",
      "fold: 1 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 835517.5813 val_loss: 654743.0000\n",
      "Epoch2: loss: 280514.8598 val_loss: 49317.8125\n",
      "Epoch3: loss: 82605.8807 val_loss: 23976.8105\n",
      "Epoch4: loss: 38653.1141 val_loss: 25545.0938\n",
      "Epoch5: loss: 20582.1848 val_loss: 11041.4355\n",
      "Epoch6: loss: 14260.0101 val_loss: 18765.0879\n",
      "Epoch7: loss: 13396.7784 val_loss: 9370.5234\n",
      "Epoch8: loss: 10975.6043 val_loss: 11475.2578\n",
      "Epoch9: loss: 9934.3671 val_loss: 8098.0908\n",
      "Epoch10: loss: 9300.2553 val_loss: 10014.5664\n",
      "Epoch11: loss: 8548.9701 val_loss: 8147.7285\n",
      "Epoch12: loss: 8100.6586 val_loss: 8749.0254\n",
      "Epoch13: loss: 7495.9875 val_loss: 8519.0732\n",
      "Epoch14: loss: 7049.0375 val_loss: 8613.2217\n",
      "Epoch15: loss: 6594.6847 val_loss: 7802.8687\n",
      "Epoch16: loss: 6233.1500 val_loss: 8300.7383\n",
      "Epoch17: loss: 5933.2735 val_loss: 7666.0874\n",
      "Epoch18: loss: 5677.1260 val_loss: 7284.1045\n",
      "Epoch19: loss: 5396.1848 val_loss: 8467.7949\n",
      "Epoch20: loss: 5119.6418 val_loss: 7373.9819\n",
      "Epoch21: loss: 4951.1935 val_loss: 7518.6206\n",
      "Epoch22: loss: 4748.5969 val_loss: 8189.4404\n",
      "Epoch23: loss: 4508.7181 val_loss: 6920.7866\n",
      "Epoch24: loss: 4432.0835 val_loss: 8650.9609\n",
      "Epoch25: loss: 4229.5258 val_loss: 6746.9502\n",
      "Epoch26: loss: 4141.9938 val_loss: 6936.3921\n",
      "Epoch27: loss: 4110.6728 val_loss: 8197.2969\n",
      "Epoch28: loss: 3834.6613 val_loss: 7416.2148\n",
      "Epoch29: loss: 3672.2322 val_loss: 7198.2510\n",
      "Epoch30: loss: 3624.6552 val_loss: 6072.9727\n",
      "Epoch31: loss: 3785.0426 val_loss: 6951.0938\n",
      "Epoch32: loss: 3509.6626 val_loss: 6821.9561\n",
      "Epoch33: loss: 3539.2348 val_loss: 7766.9990\n",
      "Epoch34: loss: 3444.0687 val_loss: 8196.3203\n",
      "Epoch35: loss: 3334.9477 val_loss: 6119.6025\n",
      "Epoch36: loss: 3215.7377 val_loss: 5894.0459\n",
      "Epoch37: loss: 3085.6194 val_loss: 8319.9873\n",
      "Epoch38: loss: 3021.7083 val_loss: 5804.6797\n",
      "Epoch39: loss: 3056.2564 val_loss: 6143.6929\n",
      "Epoch40: loss: 2969.8885 val_loss: 5785.5039\n",
      "Epoch41: loss: 2889.2703 val_loss: 5994.8467\n",
      "Epoch42: loss: 2878.0624 val_loss: 7238.1553\n",
      "Epoch43: loss: 2868.1435 val_loss: 6670.0205\n",
      "Epoch44: loss: 2813.2323 val_loss: 5411.8794\n",
      "Epoch45: loss: 2612.5120 val_loss: 5870.0483\n",
      "Epoch46: loss: 2620.3177 val_loss: 5803.3223\n",
      "Epoch47: loss: 2602.5376 val_loss: 5384.4053\n",
      "Epoch48: loss: 2557.9117 val_loss: 5649.6543\n",
      "Epoch49: loss: 2503.8807 val_loss: 6119.4414\n",
      "Epoch50: loss: 2654.2319 val_loss: 5951.0874\n",
      "Epoch51: loss: 2634.7290 val_loss: 4939.8086\n",
      "Epoch52: loss: 2402.9780 val_loss: 4897.0713\n",
      "Epoch53: loss: 2450.8394 val_loss: 5389.1816\n",
      "Epoch54: loss: 2319.1731 val_loss: 4989.7954\n",
      "Epoch55: loss: 2326.7345 val_loss: 6404.0601\n",
      "Epoch56: loss: 2428.1451 val_loss: 5276.5903\n",
      "Epoch57: loss: 2261.0110 val_loss: 5763.6260\n",
      "Epoch58: loss: 2236.8084 val_loss: 4929.2363\n",
      "Epoch59: loss: 2228.3500 val_loss: 5460.7397\n",
      "Epoch60: loss: 2155.1391 val_loss: 4623.4170\n",
      "Epoch61: loss: 2167.6277 val_loss: 5062.0996\n",
      "Epoch62: loss: 2156.3060 val_loss: 5511.0918\n",
      "Epoch63: loss: 2250.6011 val_loss: 4779.8887\n",
      "Epoch64: loss: 2100.8533 val_loss: 5274.4575\n",
      "Epoch65: loss: 2181.3252 val_loss: 3960.7173\n",
      "Epoch66: loss: 2093.5683 val_loss: 4687.6523\n",
      "Epoch67: loss: 2104.9462 val_loss: 4307.9238\n",
      "Epoch68: loss: 2061.0029 val_loss: 5448.3042\n",
      "Epoch69: loss: 2203.3146 val_loss: 4360.0771\n",
      "Epoch70: loss: 1942.0588 val_loss: 4359.4307\n",
      "Epoch71: loss: 1959.0536 val_loss: 4873.7798\n",
      "Epoch72: loss: 2048.8169 val_loss: 5373.4883\n",
      "Epoch73: loss: 2006.4736 val_loss: 4762.5459\n",
      "Epoch74: loss: 1999.2816 val_loss: 3950.6270\n",
      "Epoch75: loss: 1905.3538 val_loss: 3546.5913\n",
      "Epoch76: loss: 1957.9097 val_loss: 3455.4482\n",
      "Epoch77: loss: 2049.2989 val_loss: 3560.0359\n",
      "Epoch78: loss: 1933.7669 val_loss: 3810.6670\n",
      "Epoch79: loss: 1992.2224 val_loss: 3554.4363\n",
      "Epoch80: loss: 1869.4801 val_loss: 3861.3354\n",
      "Epoch81: loss: 1783.0214 val_loss: 3766.3679\n",
      "Epoch82: loss: 1749.5937 val_loss: 4656.5532\n",
      "Epoch83: loss: 1778.2330 val_loss: 3885.2419\n",
      "Epoch84: loss: 1718.5875 val_loss: 3821.8103\n",
      "Epoch85: loss: 1744.3315 val_loss: 3474.9580\n",
      "Epoch86: loss: 1741.9290 val_loss: 3949.8979\n",
      "Epoch87: loss: 1774.1509 val_loss: 4353.7812\n",
      "Epoch88: loss: 1804.7214 val_loss: 5287.6201\n",
      "Epoch89: loss: 1914.4488 val_loss: 4669.6475\n",
      "Epoch90: loss: 1771.6308 val_loss: 4496.0405\n",
      "Epoch91: loss: 1742.3814 val_loss: 3781.3992\n",
      "Epoch92: loss: 1635.7100 val_loss: 3727.3359\n",
      "Epoch93: loss: 1546.3786 val_loss: 3466.2290\n",
      "Epoch94: loss: 1537.7398 val_loss: 3725.1812\n",
      "Epoch95: loss: 1503.0562 val_loss: 3328.4800\n",
      "Epoch96: loss: 1528.3177 val_loss: 3302.6960\n",
      "Epoch97: loss: 1601.7158 val_loss: 3673.4478\n",
      "Epoch98: loss: 1505.5637 val_loss: 4344.7070\n",
      "Epoch99: loss: 1501.4285 val_loss: 3910.1157\n",
      "Epoch100: loss: 1478.3161 val_loss: 3594.0830\n",
      "Epoch101: loss: 1440.0855 val_loss: 4238.5645\n",
      "Epoch102: loss: 1640.5450 val_loss: 3568.6865\n",
      "Epoch103: loss: 1459.3522 val_loss: 3489.0913\n",
      "Epoch104: loss: 1427.5925 val_loss: 3203.2195\n",
      "Epoch105: loss: 1474.4806 val_loss: 3400.2378\n",
      "Epoch106: loss: 1388.8102 val_loss: 3328.5388\n",
      "Epoch107: loss: 1369.6814 val_loss: 3736.3896\n",
      "Epoch108: loss: 1367.0931 val_loss: 3749.1987\n",
      "Epoch109: loss: 1279.6957 val_loss: 3336.8962\n",
      "Epoch110: loss: 1361.2726 val_loss: 3156.8455\n",
      "Epoch111: loss: 1322.2445 val_loss: 3509.1316\n",
      "Epoch112: loss: 1244.7275 val_loss: 3444.0410\n",
      "Epoch113: loss: 1255.6082 val_loss: 3585.1108\n",
      "Epoch114: loss: 1197.3453 val_loss: 3648.7017\n",
      "Epoch115: loss: 1255.9994 val_loss: 3634.3005\n",
      "Epoch116: loss: 1180.5303 val_loss: 3428.9795\n",
      "Epoch117: loss: 1183.9614 val_loss: 3414.7537\n",
      "Epoch118: loss: 1142.3504 val_loss: 3623.8911\n",
      "Epoch119: loss: 1133.7894 val_loss: 3380.2358\n",
      "Epoch120: loss: 1127.8190 val_loss: 3534.1792\n",
      "Epoch121: loss: 1137.4252 val_loss: 3657.7925\n",
      "Epoch122: loss: 1125.7138 val_loss: 3577.1172\n",
      "Epoch123: loss: 1096.7416 val_loss: 3274.6680\n",
      "Epoch124: loss: 1216.4191 val_loss: 3474.7266\n",
      "Epoch125: loss: 1312.2997 val_loss: 4322.7383\n",
      "Epoch126: loss: 1148.8557 val_loss: 3520.2969\n",
      "Epoch127: loss: 1113.7248 val_loss: 3778.8301\n",
      "Epoch128: loss: 1021.5064 val_loss: 3946.4734\n",
      "Epoch129: loss: 1016.0956 val_loss: 3707.4922\n",
      "Epoch130: loss: 989.6675 val_loss: 3845.7468\n",
      "Epoch131: loss: 970.6791 val_loss: 3476.8650\n",
      "Early stopped training at epoch 131\n",
      "Accuracy of the network on the 318 train images: 70.8 %\n",
      "Accuracy of the network on the 24 val images: 41.7 %\n",
      "fold: 2 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 831853.8938 val_loss: 628413.1875\n",
      "Epoch2: loss: 269323.1941 val_loss: 124842.2422\n",
      "Epoch3: loss: 88556.5965 val_loss: 36090.1094\n",
      "Epoch4: loss: 46593.2762 val_loss: 31533.5527\n",
      "Epoch5: loss: 21337.9346 val_loss: 29283.9141\n",
      "Epoch6: loss: 14982.4389 val_loss: 19010.3496\n",
      "Epoch7: loss: 12585.7163 val_loss: 14978.7637\n",
      "Epoch8: loss: 11182.7889 val_loss: 12969.1396\n",
      "Epoch9: loss: 10299.6280 val_loss: 12271.6904\n",
      "Epoch10: loss: 9261.5015 val_loss: 10871.3936\n",
      "Epoch11: loss: 8852.8294 val_loss: 9924.8398\n",
      "Epoch12: loss: 8266.1093 val_loss: 9189.7266\n",
      "Epoch13: loss: 7732.6405 val_loss: 8479.9248\n",
      "Epoch14: loss: 7434.4188 val_loss: 7725.4922\n",
      "Epoch15: loss: 6918.0109 val_loss: 7171.0850\n",
      "Epoch16: loss: 6500.2667 val_loss: 6935.7148\n",
      "Epoch17: loss: 6377.1217 val_loss: 6384.5132\n",
      "Epoch18: loss: 6052.5963 val_loss: 5892.2358\n",
      "Epoch19: loss: 5821.4997 val_loss: 5796.1758\n",
      "Epoch20: loss: 5729.6985 val_loss: 5581.7188\n",
      "Epoch21: loss: 5249.7145 val_loss: 5266.8022\n",
      "Epoch22: loss: 5130.5065 val_loss: 5160.5654\n",
      "Epoch23: loss: 4898.1177 val_loss: 4924.2041\n",
      "Epoch24: loss: 4688.0559 val_loss: 4620.2598\n",
      "Epoch25: loss: 4544.8216 val_loss: 4523.0112\n",
      "Epoch26: loss: 4378.3562 val_loss: 4509.8379\n",
      "Epoch27: loss: 4230.5669 val_loss: 4353.2339\n",
      "Epoch28: loss: 4094.7771 val_loss: 4395.1709\n",
      "Epoch29: loss: 3930.5077 val_loss: 4080.7332\n",
      "Epoch30: loss: 3824.0684 val_loss: 4169.4111\n",
      "Epoch31: loss: 3760.6439 val_loss: 4040.1436\n",
      "Epoch32: loss: 3607.6579 val_loss: 4088.4031\n",
      "Epoch33: loss: 3502.3014 val_loss: 3907.6323\n",
      "Epoch34: loss: 3381.6698 val_loss: 3893.1113\n",
      "Epoch35: loss: 3421.5399 val_loss: 4019.7019\n",
      "Epoch36: loss: 3442.6772 val_loss: 3909.1062\n",
      "Epoch37: loss: 3341.2653 val_loss: 3722.4363\n",
      "Epoch38: loss: 3253.7700 val_loss: 3803.1904\n",
      "Epoch39: loss: 3023.5927 val_loss: 3769.9385\n",
      "Epoch40: loss: 3019.7334 val_loss: 3627.2048\n",
      "Epoch41: loss: 2948.9896 val_loss: 3836.6160\n",
      "Epoch42: loss: 2972.6354 val_loss: 3610.7468\n",
      "Epoch43: loss: 3102.2015 val_loss: 4073.5869\n",
      "Epoch44: loss: 3222.0169 val_loss: 3944.8979\n",
      "Epoch45: loss: 3019.4465 val_loss: 4020.2246\n",
      "Epoch46: loss: 2818.0172 val_loss: 3785.0410\n",
      "Epoch47: loss: 2736.0795 val_loss: 3651.2886\n",
      "Epoch48: loss: 2652.8282 val_loss: 3431.6230\n",
      "Epoch49: loss: 2507.0708 val_loss: 3411.4080\n",
      "Epoch50: loss: 2497.3599 val_loss: 3851.0740\n",
      "Epoch51: loss: 2585.1991 val_loss: 3524.4492\n",
      "Epoch52: loss: 2549.2835 val_loss: 3365.7627\n",
      "Epoch53: loss: 2492.9814 val_loss: 3346.2966\n",
      "Epoch54: loss: 2439.9675 val_loss: 3510.2686\n",
      "Epoch55: loss: 2403.1446 val_loss: 3292.8018\n",
      "Epoch56: loss: 2338.4236 val_loss: 3301.9031\n",
      "Epoch57: loss: 2357.0789 val_loss: 3240.1111\n",
      "Epoch58: loss: 2220.6535 val_loss: 3235.2195\n",
      "Epoch59: loss: 2221.7455 val_loss: 3382.2212\n",
      "Epoch60: loss: 2164.9315 val_loss: 3218.3901\n",
      "Epoch61: loss: 2199.2926 val_loss: 3230.0996\n",
      "Epoch62: loss: 2111.9932 val_loss: 3222.5913\n",
      "Epoch63: loss: 2130.3947 val_loss: 3401.5991\n",
      "Epoch64: loss: 2070.1978 val_loss: 3495.8755\n",
      "Epoch65: loss: 2120.8910 val_loss: 3281.0554\n",
      "Epoch66: loss: 2279.9692 val_loss: 3180.5078\n",
      "Epoch67: loss: 2138.4754 val_loss: 3597.2244\n",
      "Epoch68: loss: 2212.1557 val_loss: 3153.3394\n",
      "Epoch69: loss: 1984.8452 val_loss: 3185.7009\n",
      "Epoch70: loss: 1984.5774 val_loss: 3131.1665\n",
      "Epoch71: loss: 2009.7128 val_loss: 3181.6885\n",
      "Epoch72: loss: 1992.6245 val_loss: 3133.1084\n",
      "Epoch73: loss: 1882.7224 val_loss: 3233.9395\n",
      "Epoch74: loss: 1890.8984 val_loss: 3144.2954\n",
      "Epoch75: loss: 1884.3812 val_loss: 3160.1648\n",
      "Epoch76: loss: 1822.1264 val_loss: 3423.7744\n",
      "Epoch77: loss: 1925.6196 val_loss: 4139.9917\n",
      "Epoch78: loss: 2015.1664 val_loss: 3346.9858\n",
      "Epoch79: loss: 1847.8355 val_loss: 3132.8926\n",
      "Epoch80: loss: 1819.2439 val_loss: 3229.5977\n",
      "Epoch81: loss: 1760.5548 val_loss: 3245.0269\n",
      "Epoch82: loss: 1847.9631 val_loss: 3151.4844\n",
      "Epoch83: loss: 1814.5482 val_loss: 3102.8718\n",
      "Epoch84: loss: 1888.3579 val_loss: 3091.6113\n",
      "Epoch85: loss: 1667.6853 val_loss: 3131.8945\n",
      "Epoch86: loss: 1753.4257 val_loss: 3178.0378\n",
      "Epoch87: loss: 1838.1035 val_loss: 3204.1882\n",
      "Epoch88: loss: 1690.9292 val_loss: 3256.6172\n",
      "Epoch89: loss: 1761.8996 val_loss: 3183.8799\n",
      "Epoch90: loss: 1852.1912 val_loss: 3602.7988\n",
      "Epoch91: loss: 1794.3709 val_loss: 3120.5176\n",
      "Epoch92: loss: 1637.2839 val_loss: 3311.1304\n",
      "Epoch93: loss: 1755.8605 val_loss: 3251.5417\n",
      "Epoch94: loss: 1652.3821 val_loss: 3713.6899\n",
      "Epoch95: loss: 1698.9339 val_loss: 3333.8296\n",
      "Epoch96: loss: 1596.7754 val_loss: 3164.1667\n",
      "Epoch97: loss: 1567.8889 val_loss: 3174.5562\n",
      "Epoch98: loss: 1538.4656 val_loss: 3160.0913\n",
      "Epoch99: loss: 1556.4892 val_loss: 3123.1406\n",
      "Epoch100: loss: 1596.4368 val_loss: 3123.8960\n",
      "Epoch101: loss: 1531.9215 val_loss: 3199.9451\n",
      "Epoch102: loss: 1654.6050 val_loss: 3157.1909\n",
      "Epoch103: loss: 1477.8865 val_loss: 3304.4841\n",
      "Epoch104: loss: 1608.6439 val_loss: 3905.8640\n",
      "Epoch105: loss: 1816.6882 val_loss: 3676.9858\n",
      "Early stopped training at epoch 105\n",
      "Accuracy of the network on the 315 train images: 58.4 %\n",
      "Accuracy of the network on the 24 val images: 54.2 %\n",
      "fold: 3 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 855949.2625 val_loss: 725797.7500\n",
      "Epoch2: loss: 381103.9598 val_loss: 25013.3184\n",
      "Epoch3: loss: 87147.8199 val_loss: 17453.2031\n",
      "Epoch4: loss: 36352.1420 val_loss: 46854.4492\n",
      "Epoch5: loss: 24041.9988 val_loss: 16096.8867\n",
      "Epoch6: loss: 17355.6650 val_loss: 17910.7578\n",
      "Epoch7: loss: 14237.9835 val_loss: 10656.2656\n",
      "Epoch8: loss: 12325.2447 val_loss: 11398.4375\n",
      "Epoch9: loss: 10993.9824 val_loss: 10742.2109\n",
      "Epoch10: loss: 9852.6668 val_loss: 10383.8877\n",
      "Epoch11: loss: 9248.8583 val_loss: 10120.5205\n",
      "Epoch12: loss: 8724.1589 val_loss: 9686.8867\n",
      "Epoch13: loss: 8022.4556 val_loss: 9602.3340\n",
      "Epoch14: loss: 7576.8776 val_loss: 9223.2637\n",
      "Epoch15: loss: 7286.2916 val_loss: 8540.6963\n",
      "Epoch16: loss: 6889.3282 val_loss: 9155.3770\n",
      "Epoch17: loss: 6645.7895 val_loss: 9184.2158\n",
      "Epoch18: loss: 6037.3470 val_loss: 7967.3672\n",
      "Epoch19: loss: 5835.6068 val_loss: 8052.8613\n",
      "Epoch20: loss: 5377.3356 val_loss: 7939.4668\n",
      "Epoch21: loss: 5128.5529 val_loss: 7483.8491\n",
      "Epoch22: loss: 5000.0047 val_loss: 8926.5957\n",
      "Epoch23: loss: 5023.0350 val_loss: 7317.5796\n",
      "Epoch24: loss: 4470.1378 val_loss: 7335.2319\n",
      "Epoch25: loss: 4246.1242 val_loss: 7615.4639\n",
      "Epoch26: loss: 4142.4950 val_loss: 6998.4287\n",
      "Epoch27: loss: 4009.1568 val_loss: 7300.1206\n",
      "Epoch28: loss: 3871.1883 val_loss: 7751.2476\n",
      "Epoch29: loss: 3869.9766 val_loss: 7146.4775\n",
      "Epoch30: loss: 3563.1986 val_loss: 6780.1748\n",
      "Epoch31: loss: 3719.6296 val_loss: 7696.3789\n",
      "Epoch32: loss: 3422.5630 val_loss: 7050.4136\n",
      "Epoch33: loss: 3308.8622 val_loss: 7034.0347\n",
      "Epoch34: loss: 3230.6878 val_loss: 6667.0352\n",
      "Epoch35: loss: 3266.1367 val_loss: 6932.2041\n",
      "Epoch36: loss: 3086.0409 val_loss: 7043.0718\n",
      "Epoch37: loss: 2983.2375 val_loss: 7022.3740\n",
      "Epoch38: loss: 2940.0376 val_loss: 7305.8545\n",
      "Epoch39: loss: 2811.4682 val_loss: 6586.9819\n",
      "Epoch40: loss: 2714.7369 val_loss: 7497.3428\n",
      "Epoch41: loss: 2863.1618 val_loss: 6782.1904\n",
      "Epoch42: loss: 2706.3393 val_loss: 6583.1953\n",
      "Epoch43: loss: 2680.2832 val_loss: 6645.6445\n",
      "Epoch44: loss: 2589.9212 val_loss: 6963.0308\n",
      "Epoch45: loss: 2538.8051 val_loss: 6483.1416\n",
      "Epoch46: loss: 2593.4927 val_loss: 6466.3608\n",
      "Epoch47: loss: 2516.3337 val_loss: 6449.8516\n",
      "Epoch48: loss: 2427.3360 val_loss: 6503.8828\n",
      "Epoch49: loss: 2377.8135 val_loss: 6808.8164\n",
      "Epoch50: loss: 2310.0066 val_loss: 6443.9077\n",
      "Epoch51: loss: 2425.8456 val_loss: 6365.0430\n",
      "Epoch52: loss: 2238.6183 val_loss: 7024.4546\n",
      "Epoch53: loss: 2257.5134 val_loss: 6476.1582\n",
      "Epoch54: loss: 2196.5751 val_loss: 6384.6304\n",
      "Epoch55: loss: 2212.3162 val_loss: 6211.3496\n",
      "Epoch56: loss: 2264.0650 val_loss: 6643.7412\n",
      "Epoch57: loss: 2281.5767 val_loss: 7653.7051\n",
      "Epoch58: loss: 2214.0995 val_loss: 6800.0586\n",
      "Epoch59: loss: 2206.3105 val_loss: 6359.2402\n",
      "Epoch60: loss: 2007.2354 val_loss: 6277.1089\n",
      "Epoch61: loss: 2065.5140 val_loss: 6631.2344\n",
      "Epoch62: loss: 2014.5435 val_loss: 6653.0508\n",
      "Epoch63: loss: 1979.8193 val_loss: 6596.2671\n",
      "Epoch64: loss: 2036.8508 val_loss: 6108.3037\n",
      "Epoch65: loss: 2045.1722 val_loss: 6114.9990\n",
      "Epoch66: loss: 1878.0538 val_loss: 6745.8643\n",
      "Epoch67: loss: 1896.1551 val_loss: 6467.0093\n",
      "Epoch68: loss: 2015.4273 val_loss: 6210.0854\n",
      "Epoch69: loss: 1934.8920 val_loss: 6364.0635\n",
      "Epoch70: loss: 2187.7873 val_loss: 6067.8628\n",
      "Epoch71: loss: 2128.6366 val_loss: 6071.6304\n",
      "Epoch72: loss: 1954.9283 val_loss: 6034.0537\n",
      "Epoch73: loss: 1927.9435 val_loss: 6047.1328\n",
      "Epoch74: loss: 1912.7431 val_loss: 6051.7617\n",
      "Epoch75: loss: 1864.8336 val_loss: 6359.4116\n",
      "Epoch76: loss: 1745.3659 val_loss: 6208.1401\n",
      "Epoch77: loss: 1731.2235 val_loss: 6052.4097\n",
      "Epoch78: loss: 1728.4248 val_loss: 6001.4590\n",
      "Epoch79: loss: 1735.0004 val_loss: 6253.3530\n",
      "Epoch80: loss: 1700.2179 val_loss: 6902.0498\n",
      "Epoch81: loss: 1745.1792 val_loss: 6456.1709\n",
      "Epoch82: loss: 1674.9865 val_loss: 6190.8770\n",
      "Epoch83: loss: 1713.4229 val_loss: 6379.1602\n",
      "Epoch84: loss: 1680.9498 val_loss: 6503.5566\n",
      "Epoch85: loss: 1749.7768 val_loss: 6507.0947\n",
      "Epoch86: loss: 1754.0586 val_loss: 6513.7397\n",
      "Epoch87: loss: 1891.1646 val_loss: 6379.0815\n",
      "Epoch88: loss: 1610.7270 val_loss: 6368.7188\n",
      "Epoch89: loss: 1670.9380 val_loss: 6261.9375\n",
      "Epoch90: loss: 1629.0726 val_loss: 5824.3784\n",
      "Epoch91: loss: 1906.6956 val_loss: 5871.1133\n",
      "Epoch92: loss: 1864.9209 val_loss: 6172.8311\n",
      "Epoch93: loss: 1658.6377 val_loss: 6126.8076\n",
      "Epoch94: loss: 1669.2746 val_loss: 7127.0459\n",
      "Epoch95: loss: 1567.0331 val_loss: 6507.8711\n",
      "Epoch96: loss: 1609.3614 val_loss: 6003.7930\n",
      "Epoch97: loss: 1554.6335 val_loss: 5739.1030\n",
      "Epoch98: loss: 1525.0435 val_loss: 5906.0752\n",
      "Epoch99: loss: 1534.8821 val_loss: 5767.7549\n",
      "Epoch100: loss: 1524.9358 val_loss: 5702.3589\n",
      "Epoch101: loss: 1526.0838 val_loss: 5881.9688\n",
      "Epoch102: loss: 1461.9701 val_loss: 5713.7964\n",
      "Epoch103: loss: 1506.8954 val_loss: 6269.1709\n",
      "Epoch104: loss: 1525.1685 val_loss: 5942.3633\n",
      "Epoch105: loss: 1480.3679 val_loss: 5918.5752\n",
      "Epoch106: loss: 1444.2002 val_loss: 6131.5596\n",
      "Epoch107: loss: 1436.1179 val_loss: 6836.2344\n",
      "Epoch108: loss: 1552.9955 val_loss: 6303.2769\n",
      "Epoch109: loss: 1447.4094 val_loss: 5816.4854\n",
      "Epoch110: loss: 1488.8798 val_loss: 5745.5078\n",
      "Epoch111: loss: 1738.4350 val_loss: 5639.6982\n",
      "Epoch112: loss: 2025.7750 val_loss: 5491.4766\n",
      "Epoch113: loss: 1614.0392 val_loss: 5348.2612\n",
      "Epoch114: loss: 1554.1906 val_loss: 5580.0312\n",
      "Epoch115: loss: 1445.7451 val_loss: 5858.9351\n",
      "Epoch116: loss: 1378.9385 val_loss: 5704.1006\n",
      "Epoch117: loss: 1364.0635 val_loss: 5884.1631\n",
      "Epoch118: loss: 1352.0140 val_loss: 6465.3296\n",
      "Epoch119: loss: 1360.3293 val_loss: 5676.5156\n",
      "Epoch120: loss: 1336.0771 val_loss: 5524.8218\n",
      "Epoch121: loss: 1395.8813 val_loss: 5376.8379\n",
      "Epoch122: loss: 1424.5646 val_loss: 5983.5439\n",
      "Epoch123: loss: 1382.8441 val_loss: 5925.0405\n",
      "Epoch124: loss: 1332.7972 val_loss: 5447.5986\n",
      "Epoch125: loss: 1356.8945 val_loss: 5512.4795\n",
      "Epoch126: loss: 1354.7671 val_loss: 5936.3960\n",
      "Epoch127: loss: 1351.5292 val_loss: 5610.4873\n",
      "Epoch128: loss: 1316.7138 val_loss: 5384.5464\n",
      "Epoch129: loss: 1313.7344 val_loss: 5406.4873\n",
      "Epoch130: loss: 1239.4659 val_loss: 5462.5107\n",
      "Epoch131: loss: 1222.4498 val_loss: 6248.0132\n",
      "Epoch132: loss: 1313.5372 val_loss: 5682.0820\n",
      "Epoch133: loss: 1292.4640 val_loss: 5276.2319\n",
      "Epoch134: loss: 1239.6038 val_loss: 5316.2134\n",
      "Epoch135: loss: 1416.1511 val_loss: 6050.4155\n",
      "Epoch136: loss: 1348.8196 val_loss: 5492.3076\n",
      "Epoch137: loss: 1194.7132 val_loss: 5277.1787\n",
      "Epoch138: loss: 1151.2027 val_loss: 5461.5420\n",
      "Epoch139: loss: 1156.0858 val_loss: 5284.1338\n",
      "Epoch140: loss: 1266.4634 val_loss: 5311.5947\n",
      "Epoch141: loss: 1281.5991 val_loss: 5547.2339\n",
      "Epoch142: loss: 1104.9108 val_loss: 5787.8218\n",
      "Epoch143: loss: 1135.6584 val_loss: 5697.9624\n",
      "Epoch144: loss: 1108.1529 val_loss: 5455.8037\n",
      "Epoch145: loss: 1109.0752 val_loss: 5245.0649\n",
      "Epoch146: loss: 1087.0217 val_loss: 5386.1904\n",
      "Epoch147: loss: 1075.1332 val_loss: 5540.4619\n",
      "Epoch148: loss: 1140.9772 val_loss: 6064.0576\n",
      "Epoch149: loss: 1132.5924 val_loss: 5336.5601\n",
      "Epoch150: loss: 1134.2057 val_loss: 5199.2441\n",
      "Epoch151: loss: 1107.6305 val_loss: 5741.3574\n",
      "Epoch152: loss: 1267.6468 val_loss: 5201.3232\n",
      "Epoch153: loss: 1278.1702 val_loss: 5094.5771\n",
      "Epoch154: loss: 1263.0544 val_loss: 5878.5703\n",
      "Epoch155: loss: 1369.5057 val_loss: 5624.2422\n",
      "Epoch156: loss: 1170.6249 val_loss: 5185.1870\n",
      "Epoch157: loss: 1169.5560 val_loss: 5391.2900\n",
      "Epoch158: loss: 1293.8198 val_loss: 6612.1304\n",
      "Epoch159: loss: 1281.6077 val_loss: 5294.7827\n",
      "Epoch160: loss: 1200.6124 val_loss: 5174.3154\n",
      "Epoch161: loss: 1033.0220 val_loss: 5220.9893\n",
      "Epoch162: loss: 1026.3427 val_loss: 5572.1553\n",
      "Epoch163: loss: 1035.5310 val_loss: 5197.7324\n",
      "Epoch164: loss: 1075.4324 val_loss: 5568.8926\n",
      "Epoch165: loss: 949.4868 val_loss: 5212.2202\n",
      "Epoch166: loss: 1016.8025 val_loss: 5704.5933\n",
      "Epoch167: loss: 974.5013 val_loss: 5222.8252\n",
      "Epoch168: loss: 1081.3066 val_loss: 5145.2432\n",
      "Epoch169: loss: 897.0987 val_loss: 5331.2549\n",
      "Epoch170: loss: 991.8605 val_loss: 5318.8682\n",
      "Epoch171: loss: 951.5902 val_loss: 5436.1582\n",
      "Epoch172: loss: 1190.2006 val_loss: 6361.8203\n",
      "Epoch173: loss: 1240.2701 val_loss: 5171.0210\n",
      "Epoch174: loss: 1052.1195 val_loss: 5305.9629\n",
      "Early stopped training at epoch 174\n",
      "Accuracy of the network on the 315 train images: 56.5 %\n",
      "Accuracy of the network on the 24 val images: 37.5 %\n",
      "fold: 4 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 840439.5875 val_loss: 668270.0000\n",
      "Epoch2: loss: 296848.1258 val_loss: 44858.4961\n",
      "Epoch3: loss: 113561.5746 val_loss: 22061.9062\n",
      "Epoch4: loss: 55336.3789 val_loss: 51912.4688\n",
      "Epoch5: loss: 24797.8002 val_loss: 18266.1250\n",
      "Epoch6: loss: 20446.2654 val_loss: 22955.7305\n",
      "Epoch7: loss: 14402.0650 val_loss: 11236.9893\n",
      "Epoch8: loss: 11466.1971 val_loss: 12476.0684\n",
      "Epoch9: loss: 10258.5184 val_loss: 12998.2119\n",
      "Epoch10: loss: 9683.9607 val_loss: 10989.7109\n",
      "Epoch11: loss: 9075.0265 val_loss: 10583.2627\n",
      "Epoch12: loss: 8417.0924 val_loss: 10620.3125\n",
      "Epoch13: loss: 8019.0045 val_loss: 11042.0303\n",
      "Epoch14: loss: 7548.6682 val_loss: 9878.0078\n",
      "Epoch15: loss: 7238.3000 val_loss: 9799.2051\n",
      "Epoch16: loss: 6815.4407 val_loss: 9125.9658\n",
      "Epoch17: loss: 6574.1107 val_loss: 10022.2207\n",
      "Epoch18: loss: 6188.6837 val_loss: 8590.6836\n",
      "Epoch19: loss: 5986.9525 val_loss: 8968.5547\n",
      "Epoch20: loss: 5706.6196 val_loss: 8466.7627\n",
      "Epoch21: loss: 5464.4542 val_loss: 8200.3301\n",
      "Epoch22: loss: 5322.9732 val_loss: 7605.0898\n",
      "Epoch23: loss: 5227.3311 val_loss: 7693.6211\n",
      "Epoch24: loss: 4900.7112 val_loss: 7880.6421\n",
      "Epoch25: loss: 4690.5951 val_loss: 7042.4741\n",
      "Epoch26: loss: 4542.4806 val_loss: 7011.2422\n",
      "Epoch27: loss: 4397.1551 val_loss: 6810.6753\n",
      "Epoch28: loss: 4305.5768 val_loss: 6866.4932\n",
      "Epoch29: loss: 4135.9184 val_loss: 6629.5679\n",
      "Epoch30: loss: 3963.6803 val_loss: 6448.8311\n",
      "Epoch31: loss: 3962.8482 val_loss: 6288.1475\n",
      "Epoch32: loss: 3731.7508 val_loss: 6298.4634\n",
      "Epoch33: loss: 3661.0778 val_loss: 5856.2339\n",
      "Epoch34: loss: 3635.2461 val_loss: 6672.6440\n",
      "Epoch35: loss: 3721.0721 val_loss: 6386.4443\n",
      "Epoch36: loss: 3419.1740 val_loss: 5491.3159\n",
      "Epoch37: loss: 3521.6583 val_loss: 6480.7529\n",
      "Epoch38: loss: 3371.2148 val_loss: 5521.5342\n",
      "Epoch39: loss: 3307.3223 val_loss: 5414.2031\n",
      "Epoch40: loss: 3100.1544 val_loss: 5540.0688\n",
      "Epoch41: loss: 3039.6367 val_loss: 5729.9458\n",
      "Epoch42: loss: 3007.0081 val_loss: 5117.5503\n",
      "Epoch43: loss: 3043.1849 val_loss: 5276.9873\n",
      "Epoch44: loss: 2930.6742 val_loss: 5230.0625\n",
      "Epoch45: loss: 2830.2293 val_loss: 5130.4795\n",
      "Epoch46: loss: 2806.7525 val_loss: 4967.4473\n",
      "Epoch47: loss: 3009.5579 val_loss: 5484.2554\n",
      "Epoch48: loss: 2961.1511 val_loss: 5615.1040\n",
      "Epoch49: loss: 2954.2640 val_loss: 4822.0107\n",
      "Epoch50: loss: 2867.2573 val_loss: 4742.1104\n",
      "Epoch51: loss: 2641.1495 val_loss: 4704.7773\n",
      "Epoch52: loss: 2654.2044 val_loss: 4726.1768\n",
      "Epoch53: loss: 2759.0119 val_loss: 4675.5845\n",
      "Epoch54: loss: 2594.8153 val_loss: 5335.9854\n",
      "Epoch55: loss: 2664.4022 val_loss: 4626.4561\n",
      "Epoch56: loss: 2497.4257 val_loss: 4498.5708\n",
      "Epoch57: loss: 2428.3440 val_loss: 4884.1577\n",
      "Epoch58: loss: 2481.0670 val_loss: 4712.4170\n",
      "Epoch59: loss: 2379.0177 val_loss: 4540.2495\n",
      "Epoch60: loss: 2367.5797 val_loss: 4458.7378\n",
      "Epoch61: loss: 2457.7534 val_loss: 4410.5864\n",
      "Epoch62: loss: 2310.5069 val_loss: 4362.4297\n",
      "Epoch63: loss: 2271.8153 val_loss: 4272.5244\n",
      "Epoch64: loss: 2225.0106 val_loss: 4393.2568\n",
      "Epoch65: loss: 2199.9580 val_loss: 4399.1738\n",
      "Epoch66: loss: 2253.1637 val_loss: 4446.7866\n",
      "Epoch67: loss: 2207.8840 val_loss: 4232.4482\n",
      "Epoch68: loss: 2174.1224 val_loss: 4349.7241\n",
      "Epoch69: loss: 2136.7804 val_loss: 4309.5591\n",
      "Epoch70: loss: 2256.4683 val_loss: 4166.5811\n",
      "Epoch71: loss: 2073.4722 val_loss: 4241.6045\n",
      "Epoch72: loss: 2099.3221 val_loss: 4406.4751\n",
      "Epoch73: loss: 2066.6829 val_loss: 4181.7480\n",
      "Epoch74: loss: 2051.1431 val_loss: 4084.5232\n",
      "Epoch75: loss: 2071.4789 val_loss: 3988.2305\n",
      "Epoch76: loss: 2020.4683 val_loss: 4022.9150\n",
      "Epoch77: loss: 2113.2550 val_loss: 4207.0815\n",
      "Epoch78: loss: 2049.4318 val_loss: 4009.1709\n",
      "Epoch79: loss: 1990.8642 val_loss: 4476.9375\n",
      "Epoch80: loss: 2119.3700 val_loss: 4221.5000\n",
      "Epoch81: loss: 2103.7205 val_loss: 4171.5566\n",
      "Epoch82: loss: 1887.8141 val_loss: 3916.9087\n",
      "Epoch83: loss: 1883.2249 val_loss: 4309.9590\n",
      "Epoch84: loss: 1963.9395 val_loss: 4065.4917\n",
      "Epoch85: loss: 1908.7326 val_loss: 4372.5835\n",
      "Epoch86: loss: 2000.3043 val_loss: 3955.7439\n",
      "Epoch87: loss: 1868.2903 val_loss: 3823.9817\n",
      "Epoch88: loss: 1842.8867 val_loss: 3937.9397\n",
      "Epoch89: loss: 1900.9376 val_loss: 4204.7568\n",
      "Epoch90: loss: 1905.9954 val_loss: 3927.3384\n",
      "Epoch91: loss: 1812.7411 val_loss: 3908.2432\n",
      "Epoch92: loss: 1799.2566 val_loss: 3815.4028\n",
      "Epoch93: loss: 1808.1415 val_loss: 3735.6379\n",
      "Epoch94: loss: 1783.8717 val_loss: 3838.8337\n",
      "Epoch95: loss: 1834.7186 val_loss: 4005.4824\n",
      "Epoch96: loss: 2006.3737 val_loss: 3711.2297\n",
      "Epoch97: loss: 1813.4489 val_loss: 4207.7480\n",
      "Epoch98: loss: 1832.4534 val_loss: 4057.5500\n",
      "Epoch99: loss: 1875.3116 val_loss: 4216.8291\n",
      "Epoch100: loss: 1848.6599 val_loss: 3890.8799\n",
      "Epoch101: loss: 1708.1296 val_loss: 3809.2339\n",
      "Epoch102: loss: 1688.2336 val_loss: 3805.7202\n",
      "Epoch103: loss: 1723.3031 val_loss: 4284.0933\n",
      "Epoch104: loss: 1838.7418 val_loss: 4391.9805\n",
      "Epoch105: loss: 2078.7370 val_loss: 4610.3691\n",
      "Epoch106: loss: 1854.5379 val_loss: 3776.7283\n",
      "Epoch107: loss: 1656.9259 val_loss: 3804.9001\n",
      "Epoch108: loss: 1667.0056 val_loss: 3726.5605\n",
      "Epoch109: loss: 1593.2450 val_loss: 3691.5286\n",
      "Epoch110: loss: 1646.2917 val_loss: 3847.6501\n",
      "Epoch111: loss: 1564.7512 val_loss: 3593.7412\n",
      "Epoch112: loss: 1665.1841 val_loss: 3508.3601\n",
      "Epoch113: loss: 1615.4423 val_loss: 3763.2678\n",
      "Epoch114: loss: 1648.8450 val_loss: 3640.0110\n",
      "Epoch115: loss: 1573.6707 val_loss: 3525.0371\n",
      "Epoch116: loss: 1549.5980 val_loss: 3569.8401\n",
      "Epoch117: loss: 1501.3741 val_loss: 3491.5095\n",
      "Epoch118: loss: 1595.9771 val_loss: 3588.5476\n",
      "Epoch119: loss: 1670.4707 val_loss: 3750.4492\n",
      "Epoch120: loss: 1698.1914 val_loss: 4743.9268\n",
      "Epoch121: loss: 1791.7999 val_loss: 6043.6426\n",
      "Epoch122: loss: 2317.6003 val_loss: 4842.1685\n",
      "Epoch123: loss: 1778.8659 val_loss: 4069.5847\n",
      "Epoch124: loss: 1570.1861 val_loss: 3872.7695\n",
      "Epoch125: loss: 1588.3101 val_loss: 3353.8650\n",
      "Epoch126: loss: 1487.5883 val_loss: 3394.1926\n",
      "Epoch127: loss: 1494.4364 val_loss: 3395.1672\n",
      "Epoch128: loss: 1444.5851 val_loss: 3382.2324\n",
      "Epoch129: loss: 1503.4120 val_loss: 3694.1665\n",
      "Epoch130: loss: 1492.2460 val_loss: 3405.0820\n",
      "Epoch131: loss: 1425.6833 val_loss: 3612.2759\n",
      "Epoch132: loss: 1440.4633 val_loss: 3450.7119\n",
      "Epoch133: loss: 1406.8127 val_loss: 3611.5381\n",
      "Epoch134: loss: 1410.4908 val_loss: 3402.4368\n",
      "Epoch135: loss: 1431.5887 val_loss: 3568.8242\n",
      "Epoch136: loss: 1424.6903 val_loss: 3621.7891\n",
      "Epoch137: loss: 1455.5111 val_loss: 3480.7126\n",
      "Epoch138: loss: 1366.0887 val_loss: 3206.8481\n",
      "Epoch139: loss: 1338.7755 val_loss: 3406.9163\n",
      "Epoch140: loss: 1417.3695 val_loss: 3552.8650\n",
      "Epoch141: loss: 1311.3588 val_loss: 3759.6501\n",
      "Epoch142: loss: 1513.5240 val_loss: 3490.9121\n",
      "Epoch143: loss: 1472.1365 val_loss: 3553.1292\n",
      "Epoch144: loss: 1411.9375 val_loss: 3809.9932\n",
      "Epoch145: loss: 1367.7231 val_loss: 3520.3933\n",
      "Epoch146: loss: 1326.3507 val_loss: 3591.2976\n",
      "Epoch147: loss: 1294.8745 val_loss: 3502.6919\n",
      "Epoch148: loss: 1305.6338 val_loss: 3442.8464\n",
      "Epoch149: loss: 1475.5350 val_loss: 4362.4385\n",
      "Epoch150: loss: 1290.4940 val_loss: 3728.4192\n",
      "Epoch151: loss: 1276.9954 val_loss: 3730.3716\n",
      "Epoch152: loss: 1253.9304 val_loss: 3361.5044\n",
      "Epoch153: loss: 1203.7359 val_loss: 3425.1682\n",
      "Epoch154: loss: 1176.2669 val_loss: 3496.2031\n",
      "Epoch155: loss: 1188.5578 val_loss: 3414.2292\n",
      "Epoch156: loss: 1221.5206 val_loss: 3436.6006\n",
      "Epoch157: loss: 1230.3506 val_loss: 3680.0762\n",
      "Epoch158: loss: 1141.3404 val_loss: 3401.6050\n",
      "Epoch159: loss: 1178.7161 val_loss: 3469.4541\n",
      "Early stopped training at epoch 159\n",
      "Accuracy of the network on the 315 train images: 61.3 %\n",
      "Accuracy of the network on the 23 val images: 34.8 %\n",
      "fold: 5 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 847896.7937 val_loss: 702782.0625\n",
      "Epoch2: loss: 343558.7590 val_loss: 39431.2969\n",
      "Epoch3: loss: 83456.6141 val_loss: 26562.2188\n",
      "Epoch4: loss: 33624.7031 val_loss: 37007.2344\n",
      "Epoch5: loss: 18584.3101 val_loss: 26958.2070\n",
      "Epoch6: loss: 14098.1197 val_loss: 22547.9805\n",
      "Epoch7: loss: 11247.8979 val_loss: 19246.4492\n",
      "Epoch8: loss: 9821.4304 val_loss: 17529.0371\n",
      "Epoch9: loss: 8818.2064 val_loss: 15938.1582\n",
      "Epoch10: loss: 8070.9082 val_loss: 15260.8604\n",
      "Epoch11: loss: 7388.4222 val_loss: 14165.6963\n",
      "Epoch12: loss: 6944.9987 val_loss: 13879.7178\n",
      "Epoch13: loss: 6538.5638 val_loss: 13290.9971\n",
      "Epoch14: loss: 6209.7747 val_loss: 13090.8955\n",
      "Epoch15: loss: 5850.8688 val_loss: 12413.6201\n",
      "Epoch16: loss: 5688.0465 val_loss: 12127.4326\n",
      "Epoch17: loss: 5499.2714 val_loss: 11795.1904\n",
      "Epoch18: loss: 5173.0179 val_loss: 11351.5088\n",
      "Epoch19: loss: 4842.1280 val_loss: 11540.7881\n",
      "Epoch20: loss: 4660.8437 val_loss: 10667.8164\n",
      "Epoch21: loss: 4503.3064 val_loss: 10590.5146\n",
      "Epoch22: loss: 4384.2855 val_loss: 10562.3389\n",
      "Epoch23: loss: 4103.8109 val_loss: 9933.2656\n",
      "Epoch24: loss: 3891.3807 val_loss: 10093.0693\n",
      "Epoch25: loss: 3781.6994 val_loss: 9422.1631\n",
      "Epoch26: loss: 3697.7595 val_loss: 9387.8623\n",
      "Epoch27: loss: 3503.5751 val_loss: 9344.8877\n",
      "Epoch28: loss: 3447.5089 val_loss: 8581.2920\n",
      "Epoch29: loss: 3231.9463 val_loss: 8629.6807\n",
      "Epoch30: loss: 3168.3301 val_loss: 8046.7515\n",
      "Epoch31: loss: 3100.2430 val_loss: 7985.1060\n",
      "Epoch32: loss: 3078.6436 val_loss: 7916.8120\n",
      "Epoch33: loss: 2958.6199 val_loss: 9128.2402\n",
      "Epoch34: loss: 2988.8469 val_loss: 7490.9180\n",
      "Epoch35: loss: 2825.4836 val_loss: 7135.6904\n",
      "Epoch36: loss: 2745.9974 val_loss: 7781.1177\n",
      "Epoch37: loss: 2813.0656 val_loss: 7794.4521\n",
      "Epoch38: loss: 2669.4066 val_loss: 8001.3472\n",
      "Epoch39: loss: 2644.8550 val_loss: 6661.9531\n",
      "Epoch40: loss: 2540.1720 val_loss: 6476.0605\n",
      "Epoch41: loss: 2408.0591 val_loss: 6983.8359\n",
      "Epoch42: loss: 2421.8559 val_loss: 6735.1763\n",
      "Epoch43: loss: 2318.0256 val_loss: 6212.0898\n",
      "Epoch44: loss: 2316.7895 val_loss: 5829.5176\n",
      "Epoch45: loss: 2319.1802 val_loss: 6021.5186\n",
      "Epoch46: loss: 2274.2090 val_loss: 6122.1973\n",
      "Epoch47: loss: 2158.9425 val_loss: 6445.8047\n",
      "Epoch48: loss: 2163.2674 val_loss: 6356.7954\n",
      "Epoch49: loss: 2312.6402 val_loss: 5458.4888\n",
      "Epoch50: loss: 2156.0337 val_loss: 5604.2388\n",
      "Epoch51: loss: 2221.7893 val_loss: 5360.2622\n",
      "Epoch52: loss: 2171.6359 val_loss: 5399.5664\n",
      "Epoch53: loss: 2007.6010 val_loss: 5435.7710\n",
      "Epoch54: loss: 2000.6086 val_loss: 5433.2617\n",
      "Epoch55: loss: 1957.6511 val_loss: 5424.1396\n",
      "Epoch56: loss: 1949.3187 val_loss: 6310.2759\n",
      "Epoch57: loss: 1861.3951 val_loss: 5550.4639\n",
      "Epoch58: loss: 1809.3942 val_loss: 6363.2969\n",
      "Epoch59: loss: 1932.1572 val_loss: 5837.9106\n",
      "Epoch60: loss: 1832.0694 val_loss: 5403.5156\n",
      "Epoch61: loss: 1808.3536 val_loss: 5162.7075\n",
      "Epoch62: loss: 1773.3354 val_loss: 5426.3872\n",
      "Epoch63: loss: 1718.3463 val_loss: 5480.5640\n",
      "Epoch64: loss: 1728.2275 val_loss: 5935.9146\n",
      "Epoch65: loss: 1728.8353 val_loss: 5418.1865\n",
      "Epoch66: loss: 1742.8277 val_loss: 5200.0381\n",
      "Epoch67: loss: 1671.0327 val_loss: 5406.7236\n",
      "Epoch68: loss: 1650.0152 val_loss: 5207.9214\n",
      "Epoch69: loss: 1693.8482 val_loss: 4707.9170\n",
      "Epoch70: loss: 1624.9934 val_loss: 5244.9888\n",
      "Epoch71: loss: 1619.2879 val_loss: 5332.3379\n",
      "Epoch72: loss: 1604.2874 val_loss: 4942.3525\n",
      "Epoch73: loss: 1557.8582 val_loss: 5029.4473\n",
      "Epoch74: loss: 1565.6208 val_loss: 5038.8467\n",
      "Epoch75: loss: 1561.8741 val_loss: 5891.1582\n",
      "Epoch76: loss: 1601.3593 val_loss: 5729.0342\n",
      "Epoch77: loss: 1673.1642 val_loss: 5176.9243\n",
      "Epoch78: loss: 1582.8919 val_loss: 4782.0142\n",
      "Epoch79: loss: 1506.7059 val_loss: 5142.2891\n",
      "Epoch80: loss: 1546.8459 val_loss: 4817.0225\n",
      "Epoch81: loss: 1545.0340 val_loss: 4669.4170\n",
      "Epoch82: loss: 1530.5671 val_loss: 4560.6729\n",
      "Epoch83: loss: 1444.2131 val_loss: 5083.2104\n",
      "Epoch84: loss: 1444.1179 val_loss: 5635.5566\n",
      "Epoch85: loss: 1468.4224 val_loss: 4637.4204\n",
      "Epoch86: loss: 1549.1879 val_loss: 4622.8989\n",
      "Epoch87: loss: 1463.0145 val_loss: 4880.5757\n",
      "Epoch88: loss: 1574.1050 val_loss: 4817.8208\n",
      "Epoch89: loss: 1468.5798 val_loss: 4686.9790\n",
      "Epoch90: loss: 1471.2093 val_loss: 4442.5718\n",
      "Epoch91: loss: 1403.3201 val_loss: 4975.0967\n",
      "Epoch92: loss: 1365.2469 val_loss: 4593.1079\n",
      "Epoch93: loss: 1367.9912 val_loss: 4474.0986\n",
      "Epoch94: loss: 1371.0531 val_loss: 5052.6147\n",
      "Epoch95: loss: 1300.5234 val_loss: 5123.2461\n",
      "Epoch96: loss: 1372.6895 val_loss: 4825.4058\n",
      "Epoch97: loss: 1362.4807 val_loss: 5479.4697\n",
      "Epoch98: loss: 1531.4617 val_loss: 5601.6357\n",
      "Epoch99: loss: 1318.4942 val_loss: 5498.1455\n",
      "Epoch100: loss: 1334.9444 val_loss: 4705.3018\n",
      "Epoch101: loss: 1257.8309 val_loss: 4890.3560\n",
      "Epoch102: loss: 1269.7288 val_loss: 4795.7729\n",
      "Epoch103: loss: 1223.6853 val_loss: 4754.1479\n",
      "Epoch104: loss: 1208.8297 val_loss: 4820.7905\n",
      "Epoch105: loss: 1210.7842 val_loss: 4902.3223\n",
      "Epoch106: loss: 1256.7234 val_loss: 4662.9458\n",
      "Epoch107: loss: 1273.8848 val_loss: 4335.6997\n",
      "Epoch108: loss: 1262.7891 val_loss: 5223.9717\n",
      "Epoch109: loss: 1197.8119 val_loss: 5149.6782\n",
      "Epoch110: loss: 1256.6309 val_loss: 4799.3350\n",
      "Epoch111: loss: 1194.8346 val_loss: 4715.3799\n",
      "Epoch112: loss: 1174.0892 val_loss: 4671.4878\n",
      "Epoch113: loss: 1187.4527 val_loss: 4389.0972\n",
      "Epoch114: loss: 1248.6887 val_loss: 4381.3506\n",
      "Epoch115: loss: 1295.7769 val_loss: 5372.2822\n",
      "Epoch116: loss: 1182.9064 val_loss: 4779.4878\n",
      "Epoch117: loss: 1130.4633 val_loss: 5636.0869\n",
      "Epoch118: loss: 1164.7112 val_loss: 4564.3872\n",
      "Epoch119: loss: 1172.8854 val_loss: 4915.2500\n",
      "Epoch120: loss: 1125.3687 val_loss: 4389.0854\n",
      "Epoch121: loss: 1290.3140 val_loss: 5717.7231\n",
      "Epoch122: loss: 1132.7716 val_loss: 4950.9858\n",
      "Epoch123: loss: 1088.1883 val_loss: 5410.9253\n",
      "Epoch124: loss: 1034.3232 val_loss: 4887.8711\n",
      "Epoch125: loss: 1131.7668 val_loss: 5446.5767\n",
      "Epoch126: loss: 1127.8138 val_loss: 6164.3438\n",
      "Epoch127: loss: 1211.4098 val_loss: 5620.8242\n",
      "Epoch128: loss: 1074.8628 val_loss: 5112.2495\n",
      "Early stopped training at epoch 128\n",
      "Accuracy of the network on the 315 train images: 71.4 %\n",
      "Accuracy of the network on the 23 val images: 26.1 %\n",
      "fold: 6 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 847921.8562 val_loss: 694163.5000\n",
      "Epoch2: loss: 334416.5568 val_loss: 24890.8535\n",
      "Epoch3: loss: 91944.1027 val_loss: 17145.0762\n",
      "Epoch4: loss: 45970.0129 val_loss: 41019.0547\n",
      "Epoch5: loss: 19621.3293 val_loss: 23311.0547\n",
      "Epoch6: loss: 16804.8659 val_loss: 18946.7539\n",
      "Epoch7: loss: 13755.8923 val_loss: 13173.5732\n",
      "Epoch8: loss: 11380.7764 val_loss: 12488.2910\n",
      "Epoch9: loss: 9950.6214 val_loss: 11650.5469\n",
      "Epoch10: loss: 9018.8290 val_loss: 10844.5889\n",
      "Epoch11: loss: 8524.7710 val_loss: 10727.1689\n",
      "Epoch12: loss: 7936.3040 val_loss: 9887.0215\n",
      "Epoch13: loss: 7359.5253 val_loss: 9481.4189\n",
      "Epoch14: loss: 6877.5938 val_loss: 9242.9688\n",
      "Epoch15: loss: 6390.3480 val_loss: 8953.0459\n",
      "Epoch16: loss: 5994.8886 val_loss: 8540.8906\n",
      "Epoch17: loss: 5642.2154 val_loss: 8368.0527\n",
      "Epoch18: loss: 5472.5885 val_loss: 8092.4238\n",
      "Epoch19: loss: 5082.4380 val_loss: 8061.7969\n",
      "Epoch20: loss: 4835.4062 val_loss: 7654.8481\n",
      "Epoch21: loss: 4593.4033 val_loss: 7769.0869\n",
      "Epoch22: loss: 4469.0514 val_loss: 7348.4497\n",
      "Epoch23: loss: 4240.2964 val_loss: 7136.3110\n",
      "Epoch24: loss: 4211.1541 val_loss: 6991.1387\n",
      "Epoch25: loss: 4116.0696 val_loss: 7348.9438\n",
      "Epoch26: loss: 3947.7861 val_loss: 6742.6919\n",
      "Epoch27: loss: 3684.4318 val_loss: 6580.3452\n",
      "Epoch28: loss: 3521.4339 val_loss: 6387.8315\n",
      "Epoch29: loss: 3587.3175 val_loss: 6679.2793\n",
      "Epoch30: loss: 3552.8011 val_loss: 6460.8384\n",
      "Epoch31: loss: 3548.0689 val_loss: 6209.8804\n",
      "Epoch32: loss: 3516.0411 val_loss: 5942.2515\n",
      "Epoch33: loss: 3157.4743 val_loss: 5833.7061\n",
      "Epoch34: loss: 3065.8764 val_loss: 5614.0469\n",
      "Epoch35: loss: 3118.7237 val_loss: 5788.0415\n",
      "Epoch36: loss: 2989.7624 val_loss: 5565.4775\n",
      "Epoch37: loss: 2891.4527 val_loss: 5303.5171\n",
      "Epoch38: loss: 2931.5130 val_loss: 5276.1538\n",
      "Epoch39: loss: 2797.7544 val_loss: 5283.2310\n",
      "Epoch40: loss: 2807.9033 val_loss: 5305.2563\n",
      "Epoch41: loss: 2735.0492 val_loss: 5099.2412\n",
      "Epoch42: loss: 2647.6948 val_loss: 4892.1250\n",
      "Epoch43: loss: 2586.7458 val_loss: 4943.0454\n",
      "Epoch44: loss: 2630.8642 val_loss: 4814.0557\n",
      "Epoch45: loss: 2537.1104 val_loss: 4764.2373\n",
      "Epoch46: loss: 2534.8496 val_loss: 4938.1157\n",
      "Epoch47: loss: 2462.5102 val_loss: 4787.9438\n",
      "Epoch48: loss: 2421.4995 val_loss: 4844.4795\n",
      "Epoch49: loss: 2499.8838 val_loss: 4587.3652\n",
      "Epoch50: loss: 2367.6489 val_loss: 4492.4004\n",
      "Epoch51: loss: 2365.8076 val_loss: 4454.8223\n",
      "Epoch52: loss: 2335.3024 val_loss: 4516.0483\n",
      "Epoch53: loss: 2287.9271 val_loss: 4832.8901\n",
      "Epoch54: loss: 2204.2970 val_loss: 4367.1104\n",
      "Epoch55: loss: 2182.5810 val_loss: 4371.6357\n",
      "Epoch56: loss: 2333.1805 val_loss: 4357.8447\n",
      "Epoch57: loss: 2185.0890 val_loss: 5061.0654\n",
      "Epoch58: loss: 2190.8481 val_loss: 4345.8804\n",
      "Epoch59: loss: 2088.2504 val_loss: 4269.8872\n",
      "Epoch60: loss: 2046.1984 val_loss: 4321.4009\n",
      "Epoch61: loss: 2030.4485 val_loss: 4154.2119\n",
      "Epoch62: loss: 2366.6699 val_loss: 4472.8433\n",
      "Epoch63: loss: 2146.1900 val_loss: 4319.7930\n",
      "Epoch64: loss: 1999.4780 val_loss: 4296.4517\n",
      "Epoch65: loss: 1924.6048 val_loss: 4184.9810\n",
      "Epoch66: loss: 1876.4916 val_loss: 4229.9004\n",
      "Epoch67: loss: 1889.0543 val_loss: 4319.1074\n",
      "Epoch68: loss: 1852.9733 val_loss: 4188.4775\n",
      "Epoch69: loss: 1848.2662 val_loss: 4206.4268\n",
      "Epoch70: loss: 1813.9162 val_loss: 4217.4106\n",
      "Epoch71: loss: 1797.3245 val_loss: 4321.8765\n",
      "Epoch72: loss: 1903.3621 val_loss: 4172.8857\n",
      "Epoch73: loss: 1798.0774 val_loss: 4186.8760\n",
      "Epoch74: loss: 1825.7804 val_loss: 4166.4692\n",
      "Epoch75: loss: 1814.1194 val_loss: 4332.9038\n",
      "Epoch76: loss: 1790.5899 val_loss: 4293.1836\n",
      "Epoch77: loss: 1826.9540 val_loss: 4247.9487\n",
      "Epoch78: loss: 1786.9838 val_loss: 4946.1157\n",
      "Epoch79: loss: 1981.3344 val_loss: 4549.2568\n",
      "Epoch80: loss: 1947.3342 val_loss: 5083.1436\n",
      "Epoch81: loss: 1969.6500 val_loss: 4673.4673\n",
      "Epoch82: loss: 1717.4324 val_loss: 4868.8901\n",
      "Early stopped training at epoch 82\n",
      "Accuracy of the network on the 315 train images: 56.8 %\n",
      "Accuracy of the network on the 23 val images: 26.1 %\n",
      "fold: 7 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 845101.0062 val_loss: 680399.8125\n",
      "Epoch2: loss: 321713.5324 val_loss: 65162.0820\n",
      "Epoch3: loss: 101520.8398 val_loss: 33680.7031\n",
      "Epoch4: loss: 44657.1096 val_loss: 42260.6523\n",
      "Epoch5: loss: 23243.6017 val_loss: 34139.2383\n",
      "Epoch6: loss: 16172.1870 val_loss: 20595.0156\n",
      "Epoch7: loss: 13588.0298 val_loss: 18052.2012\n",
      "Epoch8: loss: 11134.0389 val_loss: 16290.3643\n",
      "Epoch9: loss: 10030.5935 val_loss: 15519.2031\n",
      "Epoch10: loss: 9136.1385 val_loss: 14866.2725\n",
      "Epoch11: loss: 8659.5116 val_loss: 13752.5723\n",
      "Epoch12: loss: 8359.3105 val_loss: 13410.9814\n",
      "Epoch13: loss: 7710.3032 val_loss: 12402.6602\n",
      "Epoch14: loss: 7003.9439 val_loss: 11938.6826\n",
      "Epoch15: loss: 6623.4752 val_loss: 11204.1211\n",
      "Epoch16: loss: 6254.8793 val_loss: 10749.0352\n",
      "Epoch17: loss: 6037.2281 val_loss: 10319.8535\n",
      "Epoch18: loss: 5626.7603 val_loss: 9792.0596\n",
      "Epoch19: loss: 5283.0755 val_loss: 9258.6943\n",
      "Epoch20: loss: 5005.5938 val_loss: 8623.8584\n",
      "Epoch21: loss: 4731.2223 val_loss: 8437.8184\n",
      "Epoch22: loss: 4481.2096 val_loss: 7864.8970\n",
      "Epoch23: loss: 4328.7420 val_loss: 7503.2461\n",
      "Epoch24: loss: 4164.8023 val_loss: 7522.6709\n",
      "Epoch25: loss: 3941.4388 val_loss: 6897.7861\n",
      "Epoch26: loss: 3804.3357 val_loss: 6718.3501\n",
      "Epoch27: loss: 3654.3124 val_loss: 6515.2734\n",
      "Epoch28: loss: 3612.8134 val_loss: 6165.2598\n",
      "Epoch29: loss: 3431.4775 val_loss: 5882.4790\n",
      "Epoch30: loss: 3296.8886 val_loss: 5883.1992\n",
      "Epoch31: loss: 3241.8666 val_loss: 5824.5117\n",
      "Epoch32: loss: 3149.9783 val_loss: 5741.1060\n",
      "Epoch33: loss: 3107.8042 val_loss: 5408.9253\n",
      "Epoch34: loss: 2991.9169 val_loss: 5374.3374\n",
      "Epoch35: loss: 2940.1635 val_loss: 5416.5234\n",
      "Epoch36: loss: 2873.3162 val_loss: 5303.6572\n",
      "Epoch37: loss: 2898.1121 val_loss: 5068.0957\n",
      "Epoch38: loss: 2769.4617 val_loss: 4995.7495\n",
      "Epoch39: loss: 2711.6779 val_loss: 4950.0645\n",
      "Epoch40: loss: 2695.2342 val_loss: 4949.3271\n",
      "Epoch41: loss: 2566.6825 val_loss: 4977.8101\n",
      "Epoch42: loss: 2690.6073 val_loss: 4935.6123\n",
      "Epoch43: loss: 2580.6637 val_loss: 4776.4131\n",
      "Epoch44: loss: 2441.7399 val_loss: 4811.9697\n",
      "Epoch45: loss: 2400.5999 val_loss: 4747.6743\n",
      "Epoch46: loss: 2420.5816 val_loss: 4748.7573\n",
      "Epoch47: loss: 2431.3021 val_loss: 4694.0659\n",
      "Epoch48: loss: 2379.5832 val_loss: 4714.4902\n",
      "Epoch49: loss: 2249.7652 val_loss: 4682.5620\n",
      "Epoch50: loss: 2226.4723 val_loss: 4692.8613\n",
      "Epoch51: loss: 2255.2339 val_loss: 4718.4985\n",
      "Epoch52: loss: 2237.5602 val_loss: 4788.9019\n",
      "Epoch53: loss: 2253.6398 val_loss: 4833.5166\n",
      "Epoch54: loss: 2139.7411 val_loss: 4600.7446\n",
      "Epoch55: loss: 2079.0327 val_loss: 4619.1948\n",
      "Epoch56: loss: 2138.5410 val_loss: 4835.3662\n",
      "Epoch57: loss: 2077.9578 val_loss: 4697.3423\n",
      "Epoch58: loss: 2034.0874 val_loss: 4610.8271\n",
      "Epoch59: loss: 2090.4105 val_loss: 4829.1372\n",
      "Epoch60: loss: 2097.2935 val_loss: 4587.3301\n",
      "Epoch61: loss: 2001.7781 val_loss: 4610.4604\n",
      "Epoch62: loss: 2020.5739 val_loss: 4580.6938\n",
      "Epoch63: loss: 2020.2721 val_loss: 4653.1436\n",
      "Epoch64: loss: 1875.9036 val_loss: 4622.4497\n",
      "Epoch65: loss: 1933.9621 val_loss: 4617.1851\n",
      "Epoch66: loss: 1967.2065 val_loss: 4853.7998\n",
      "Epoch67: loss: 1879.1443 val_loss: 4508.8618\n",
      "Epoch68: loss: 1859.4759 val_loss: 4688.7690\n",
      "Epoch69: loss: 1859.4456 val_loss: 4739.0049\n",
      "Epoch70: loss: 1956.2844 val_loss: 4742.7051\n",
      "Epoch71: loss: 2043.0364 val_loss: 4673.0527\n",
      "Epoch72: loss: 1916.4947 val_loss: 4580.0845\n",
      "Epoch73: loss: 1819.6148 val_loss: 4523.7246\n",
      "Epoch74: loss: 1789.6922 val_loss: 4697.7168\n",
      "Epoch75: loss: 1807.6846 val_loss: 4618.4048\n",
      "Epoch76: loss: 1758.9402 val_loss: 4588.5264\n",
      "Epoch77: loss: 1761.4522 val_loss: 4628.0366\n",
      "Epoch78: loss: 1731.2557 val_loss: 4672.9907\n",
      "Epoch79: loss: 1777.0118 val_loss: 4706.0054\n",
      "Epoch80: loss: 1797.2034 val_loss: 4615.0957\n",
      "Epoch81: loss: 1743.7601 val_loss: 4788.9414\n",
      "Epoch82: loss: 1863.6666 val_loss: 4870.9873\n",
      "Epoch83: loss: 2053.7755 val_loss: 4831.6006\n",
      "Epoch84: loss: 1752.9763 val_loss: 5102.4683\n",
      "Epoch85: loss: 1765.2949 val_loss: 4859.0229\n",
      "Epoch86: loss: 1675.9269 val_loss: 4566.6001\n",
      "Epoch87: loss: 1665.0104 val_loss: 5068.2681\n",
      "Epoch88: loss: 1890.3502 val_loss: 4816.3281\n",
      "Early stopped training at epoch 88\n",
      "Accuracy of the network on the 315 train images: 61.0 %\n",
      "Accuracy of the network on the 23 val images: 47.8 %\n",
      "fold: 8 done!\n",
      "<class 'generator'>\n",
      "Epoch1: loss: 861334.6875 val_loss: 737374.6250\n",
      "Epoch2: loss: 430056.3688 val_loss: 38746.2656\n",
      "Epoch3: loss: 89602.5848 val_loss: 56553.7617\n",
      "Epoch4: loss: 27860.5631 val_loss: 31604.8789\n",
      "Epoch5: loss: 24147.2063 val_loss: 22950.8965\n",
      "Epoch6: loss: 17326.4227 val_loss: 11204.7783\n",
      "Epoch7: loss: 13921.4542 val_loss: 10375.9814\n",
      "Epoch8: loss: 11349.6996 val_loss: 10677.2031\n",
      "Epoch9: loss: 10244.0347 val_loss: 8878.8232\n",
      "Epoch10: loss: 9351.8789 val_loss: 8437.5352\n",
      "Epoch11: loss: 8643.8614 val_loss: 8201.0498\n",
      "Epoch12: loss: 7920.1604 val_loss: 7293.1772\n",
      "Epoch13: loss: 7533.7750 val_loss: 6228.4634\n",
      "Epoch14: loss: 7227.1780 val_loss: 8275.3271\n",
      "Epoch15: loss: 6580.5367 val_loss: 5683.2051\n",
      "Epoch16: loss: 6035.1270 val_loss: 6258.6372\n",
      "Epoch17: loss: 5762.2572 val_loss: 5131.2529\n",
      "Epoch18: loss: 5361.4042 val_loss: 5885.5298\n",
      "Epoch19: loss: 5160.5708 val_loss: 5204.3232\n",
      "Epoch20: loss: 4851.9974 val_loss: 4996.8848\n",
      "Epoch21: loss: 4626.5302 val_loss: 4378.7041\n",
      "Epoch22: loss: 4529.1796 val_loss: 5446.4517\n",
      "Epoch23: loss: 4449.9848 val_loss: 5120.8018\n",
      "Epoch24: loss: 4161.7581 val_loss: 3595.6294\n",
      "Epoch25: loss: 4376.0562 val_loss: 3524.9912\n",
      "Epoch26: loss: 4249.2052 val_loss: 5771.9268\n",
      "Epoch27: loss: 3829.0045 val_loss: 3678.9895\n",
      "Epoch28: loss: 3609.6394 val_loss: 3664.2351\n",
      "Epoch29: loss: 3477.7923 val_loss: 3859.4497\n",
      "Epoch30: loss: 3442.2540 val_loss: 3663.4119\n",
      "Epoch31: loss: 3363.0121 val_loss: 3155.7959\n",
      "Epoch32: loss: 3303.0655 val_loss: 3108.2180\n",
      "Epoch33: loss: 3213.1851 val_loss: 3483.4912\n",
      "Epoch34: loss: 3225.4049 val_loss: 3491.1733\n",
      "Epoch35: loss: 3054.7480 val_loss: 3669.6631\n",
      "Epoch36: loss: 3077.1795 val_loss: 3050.4463\n",
      "Epoch37: loss: 2932.2137 val_loss: 3172.6155\n",
      "Epoch38: loss: 2874.3266 val_loss: 2875.4805\n",
      "Epoch39: loss: 2897.3012 val_loss: 4039.5554\n",
      "Epoch40: loss: 2938.5567 val_loss: 3813.2793\n",
      "Epoch41: loss: 2862.9792 val_loss: 2697.8755\n",
      "Epoch42: loss: 2776.0089 val_loss: 2984.1169\n",
      "Epoch43: loss: 2595.2667 val_loss: 3013.7690\n",
      "Epoch44: loss: 2561.3367 val_loss: 2903.0442\n",
      "Epoch45: loss: 2535.0987 val_loss: 3300.9592\n",
      "Epoch46: loss: 2488.4930 val_loss: 2845.5142\n",
      "Epoch47: loss: 2469.5689 val_loss: 2685.9390\n",
      "Epoch48: loss: 2449.7075 val_loss: 2588.2378\n",
      "Epoch49: loss: 2471.7986 val_loss: 2597.2246\n",
      "Epoch50: loss: 2382.7848 val_loss: 2749.3135\n",
      "Epoch51: loss: 2298.0242 val_loss: 2675.7153\n",
      "Epoch52: loss: 2271.7444 val_loss: 3456.8040\n",
      "Epoch53: loss: 2279.3377 val_loss: 2929.8101\n",
      "Epoch54: loss: 2239.4693 val_loss: 3571.5674\n",
      "Epoch55: loss: 2244.0466 val_loss: 2869.2725\n",
      "Epoch56: loss: 2179.4983 val_loss: 4136.1157\n",
      "Epoch57: loss: 2408.4930 val_loss: 3791.2725\n",
      "Epoch58: loss: 2298.4412 val_loss: 2876.6372\n",
      "Epoch59: loss: 2192.3256 val_loss: 2484.3621\n",
      "Epoch60: loss: 2176.7986 val_loss: 2465.6072\n",
      "Epoch61: loss: 2152.7717 val_loss: 2519.4099\n",
      "Epoch62: loss: 2033.6007 val_loss: 2483.5886\n",
      "Epoch63: loss: 2108.9452 val_loss: 2566.3096\n",
      "Epoch64: loss: 2052.7040 val_loss: 2656.9043\n",
      "Epoch65: loss: 2142.7764 val_loss: 2783.2207\n",
      "Epoch66: loss: 2002.6150 val_loss: 3256.4868\n",
      "Epoch67: loss: 1929.6106 val_loss: 2579.7148\n",
      "Epoch68: loss: 1948.4949 val_loss: 2543.3142\n",
      "Epoch69: loss: 2040.8999 val_loss: 2684.7705\n",
      "Epoch70: loss: 1927.3070 val_loss: 3431.2854\n",
      "Epoch71: loss: 1893.2426 val_loss: 4291.1401\n",
      "Epoch72: loss: 2107.7563 val_loss: 3241.2942\n",
      "Epoch73: loss: 1848.3282 val_loss: 2696.4111\n",
      "Epoch74: loss: 1917.3084 val_loss: 2613.2556\n",
      "Epoch75: loss: 1848.9134 val_loss: 2809.1445\n",
      "Epoch76: loss: 1844.0521 val_loss: 2689.9565\n",
      "Epoch77: loss: 1868.4700 val_loss: 2535.9663\n",
      "Epoch78: loss: 1995.5285 val_loss: 2604.4819\n",
      "Epoch79: loss: 1732.4302 val_loss: 3462.8792\n",
      "Epoch80: loss: 1727.2176 val_loss: 2618.8035\n",
      "Epoch81: loss: 1728.8260 val_loss: 2609.3296\n",
      "Early stopped training at epoch 81\n",
      "Accuracy of the network on the 318 train images: 62.3 %\n",
      "Accuracy of the network on the 23 val images: 69.6 %\n",
      "fold: 9 done!\n"
     ]
    }
   ],
   "source": [
    "# the paramters below were optimized\n",
    "#lr: 0.01, batch size: 16, dropout: 0.35, epochs: 32, l1: 600, l2: 50\n",
    "                        \n",
    "                        \n",
    "Learning_rate = [0.005]\n",
    "Drop_out = [0.00]#[0.0, 0.2, 0.35, 0.5]\n",
    "Batch_size = [32]#[16, 32, 100]\n",
    "L1 = [400]#[600, 400, 300, 200]\n",
    "L2 = [100]# [150, 100, 50]\n",
    "\n",
    "def cross_10_folds_mlp(train_val_X, train_val_Y):\n",
    "    #train_val_Y.reshape(len(train_val_Y),1)\n",
    "\n",
    "    best_train = []\n",
    "    best_val = []\n",
    "    best_variables = []\n",
    "    best_performance_record = []\n",
    "    for fold in range(10):\n",
    "        \n",
    "        model_path = f'regression/ImageNet/aug3/MLP/{fold}_model.pth'\n",
    "        group, train_X, train_Y, val_X, val_Y = stratified_train_test_group_kfold(train_val_X, train_val_Y, train_val_groups, n_splits=10, test_fold=fold)\n",
    "        #train_X, val_X, train_Y, val_Y = train_test_split(train_val_X, train_val_Y, test_size=0.1,stratify=train_val_Y,random_state=fold+30)#42\n",
    "        train_X, train_Y = oversample.fit_resample(train_X, train_Y)\n",
    "    \n",
    "        best_train_acc, best_val_acc, best_record, hyper=mlp_regression_gridsearch(train_X, train_Y, val_X, val_Y,Learning_rate, L1, L2, Drop_out,Batch_size, fold,model_path)\n",
    "        #best_train_acc, best_val_acc, trainf1, valf1, best_perf_record, hyper=cnn_class_train_fn(train_X, train_Y, val_X, val_Y, Learning_rate, Batch_size, fold)\n",
    "\n",
    "        best_train.append(best_train_acc)\n",
    "        best_val.append(best_val_acc)\n",
    "        best_variables.append(hyper)\n",
    "        best_performance_record.append(best_record)\n",
    "\n",
    "        print(f'fold: {fold} done!')\n",
    "    return best_train, best_val, best_variables, best_performance_record\n",
    "\n",
    "\n",
    "best_train, best_val, best_variables, best_performance_record = cross_10_folds_mlp(train_val_X, train_val_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e1bef",
   "metadata": {},
   "source": [
    "# 4. Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0bef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, l1=120, l2=84, p = 0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, l1)\n",
    "        self.fc2 = nn.Linear(l1, l2)\n",
    "        self.fc3 = nn.Linear(l2, 1)\n",
    "        #self.activ = torch.nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "l1, l2, p = 400, 100, 0.0\n",
    "trained_model = model = MLP(l1, l2, p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9c2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 27 test images: 37.0 %\n",
      "Accuracy of the network on the 27 test images: 40.7 %\n",
      "Accuracy of the network on the 27 test images: 37.0 %\n",
      "Accuracy of the network on the 27 test images: 48.1 %\n",
      "Accuracy of the network on the 27 test images: 44.4 %\n",
      "Accuracy of the network on the 27 test images: 29.6 %\n",
      "Accuracy of the network on the 27 test images: 44.4 %\n",
      "Accuracy of the network on the 27 test images: 40.7 %\n",
      "Accuracy of the network on the 27 test images: 48.1 %\n",
      "Accuracy of the network on the 27 test images: 48.1 %\n",
      "[37.03703703703704, 40.74074074074074, 37.03703703703704, 48.148148148148145, 44.44444444444444, 29.62962962962963, 44.44444444444444, 40.74074074074074, 48.148148148148145, 48.148148148148145]\n",
      "test_acc_mean: 41.852, std: 5.750\n",
      "......\n",
      "idx_cm:  [3, 8, 9]\n",
      "max_test_acc:  48.148148148148145\n",
      "[[1 1 1]\n",
      " [2 7 4]\n",
      " [3 3 5]]\n",
      "5th confusion matrix:  [[1, 1, 1], [2, 6, 5], [3, 3, 5]]\n",
      "10\n",
      "avg cm:  [[1.1, 0.9, 1.0], [3.2, 5.3, 4.5], [3.9, 2.2, 4.9]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#trained_model = pretrained_model()\n",
    "root_path = f'regression/ImageNet/aug3/MLP'\n",
    "\n",
    "best_test, confusion_matrix_test, rmse_test_folds = mlp_reg_cross_val_final_test(trained_model, test_X, test_Y, 'test', root_path)\n",
    "\n",
    "\n",
    "print(best_test)\n",
    "print(f'test_acc_mean: {np.mean(best_test) :.3f}, std: {np.std(best_test) :.3f}')\n",
    "print(\"......\")\n",
    "\n",
    "def cm_to_plot(best_test):\n",
    "\n",
    "    max_test = np.max(best_test)\n",
    "    idx_cm = []\n",
    "    for index, item in enumerate(best_test):\n",
    "        if item == max_test:\n",
    "            idx_cm.append(index)\n",
    "\n",
    "    print('idx_cm: ', idx_cm)\n",
    "    print('max_test_acc: ', max_test)\n",
    "\n",
    "    print(confusion_matrix_test[idx_cm[0]])\n",
    "cm_to_plot(best_test)\n",
    "print('5th confusion matrix: ', [list(item) for item in confusion_matrix_test[4]])\n",
    "\n",
    "\n",
    "cm_list = []\n",
    "for cm in confusion_matrix_test:\n",
    "    cm_list.append([list(item) for item in cm])\n",
    "#print(cm_list)\n",
    "\n",
    "print(len(cm_list))\n",
    "print('avg cm: ', [list(item) for item in np.mean(cm_list, axis=0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cbce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchvision] *",
   "language": "python",
   "name": "conda-env-torchvision-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
